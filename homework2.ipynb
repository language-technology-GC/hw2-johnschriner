{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a64b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This assignment will use FairSeq, a neural network sequence-to-sequence learning tool, to build an encoder-decoder LSTM grapheme-to-phoneme engine for Icelandic.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This assignment will use FairSeq, a neural network sequence-to-sequence learning tool, to build an encoder-decoder LSTM grapheme-to-phoneme engine for Icelandic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0967ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your code suggestions: I'm not entirely sure how to 'unpack the variables' if not grabbing the columns.\n",
    "I'll need to look at how to do that.\n",
    "As to closing the files, I only added that after I wasn't able to write to the file -- something was wrong, but\n",
    "once I closed it, I was able to write fine.  Very strange.\"\"\"\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "#preprocessing: checked all tsv files for missing fields\n",
    "#preparing first column of the training data for FairSeq\n",
    "trainice = open('train.ice.g', 'w')\n",
    "\n",
    "with open(\"ice_train.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for _ in reader:\n",
    "        firstcolumn = (_[0])\n",
    "        print(' '.join(firstcolumn), file = trainice)\n",
    "    trainice.close()\n",
    "    \n",
    "#your suggestion:\n",
    "#  for graphemes, _ in reader:\n",
    "#        print(' '.join(graphemes), file=trainice)\n",
    "#\n",
    "#but again, not sure how to unpack the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a4543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing second column of the training data for FairSeq - no spaces added\n",
    "\n",
    "trainice2 = open('train.ice.p', 'w')\n",
    "with open(\"ice_train.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        secondcolumn = (i[1])\n",
    "        print(secondcolumn, file = trainice2)\n",
    "    trainice2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c8f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing first column of the dev data for FairSeq\n",
    "\n",
    "device = open('dev.ice.g', 'w')\n",
    "\n",
    "with open(\"ice_dev.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        firstcolumn = (i[0])\n",
    "        print(' '.join(firstcolumn), file = device)\n",
    "    device.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4687b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing second column of the dev data for FairSeq - no spaces added\n",
    "device2 = open('dev.ice.p', 'w')\n",
    "with open(\"ice_dev.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        secondcolumn = (i[1])\n",
    "        print(secondcolumn, file = device2)\n",
    "    device2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fe31458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing first column of the test data for FairSeq\n",
    "testice = open('test.ice.g', 'w')\n",
    "\n",
    "with open(\"ice_test.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        firstcolumn = (i[0])\n",
    "        print(' '.join(firstcolumn), file = testice)\n",
    "    testice.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2dcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing first column of the test data for FairSeq - no spaces added\n",
    "testice2 = open('test.ice.p', 'w')\n",
    "with open(\"ice_test.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        secondcolumn = (i[1])\n",
    "        print(secondcolumn, file = testice2)\n",
    "    testice2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779d8d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LOG from preprocessing:\\ndeb@ubuntuuu:~/Downloads/homework2$ fairseq-preprocess >     --source-lang ice.g >     --target-lang ice.p >     --trainpref train >     --validpref dev >     --testpref test >     --tokenizer space >     --thresholdsrc 2 >     --thresholdtgt 2\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='ice.g', srcdict=None, target_lang='ice.p', task='translation', tensorboard_logdir=None, testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer='space', tpu=False, trainpref='train', user_dir=None, validpref='dev', workers=1)\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] train.ice.g: 800 sents, 5242 tokens, 0.0191% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] dev.ice.g: 100 sents, 634 tokens, 0.0% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] test.ice.g: 100 sents, 667 tokens, 0.0% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] train.ice.p: 800 sents, 5376 tokens, 0.0558% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] dev.ice.p: 100 sents, 652 tokens, 0.153% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] test.ice.p: 100 sents, 685 tokens, 0.292% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\\n\\nFiles created in data-bin:\\n\\nls -l -h| awk '{print $5, $9}'\\n \\n252 dict.ice.g.txt\\n407 dict.ice.p.txt\\n1.6K preprocess.log\\n1.4K test.ice.g-ice.p.ice.g.bin\\n1.2K test.ice.g-ice.p.ice.g.idx\\n1.4K test.ice.g-ice.p.ice.p.bin\\n1.2K test.ice.g-ice.p.ice.p.idx\\n11K train.ice.g-ice.p.ice.g.bin\\n9.5K train.ice.g-ice.p.ice.g.idx\\n11K train.ice.g-ice.p.ice.p.bin\\n9.5K train.ice.g-ice.p.ice.p.idx\\n1.3K valid.ice.g-ice.p.ice.g.bin\\n1.2K valid.ice.g-ice.p.ice.g.idx\\n1.3K valid.ice.g-ice.p.ice.p.bin\\n1.2K valid.ice.g-ice.p.ice.p.idx\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''LOG from preprocessing:\n",
    "deb@ubuntuuu:~/Downloads/homework2$ fairseq-preprocess \\\n",
    ">     --source-lang ice.g \\\n",
    ">     --target-lang ice.p \\\n",
    ">     --trainpref train \\\n",
    ">     --validpref dev \\\n",
    ">     --testpref test \\\n",
    ">     --tokenizer space \\\n",
    ">     --thresholdsrc 2 \\\n",
    ">     --thresholdtgt 2\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='ice.g', srcdict=None, target_lang='ice.p', task='translation', tensorboard_logdir=None, testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer='space', tpu=False, trainpref='train', user_dir=None, validpref='dev', workers=1)\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] train.ice.g: 800 sents, 5242 tokens, 0.0191% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] dev.ice.g: 100 sents, 634 tokens, 0.0% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] test.ice.g: 100 sents, 667 tokens, 0.0% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] train.ice.p: 800 sents, 5376 tokens, 0.0558% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] dev.ice.p: 100 sents, 652 tokens, 0.153% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] test.ice.p: 100 sents, 685 tokens, 0.292% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n",
    "\n",
    "Files created in data-bin:\n",
    "\n",
    "ls -l -h| awk '{print $5, $9}'\n",
    " \n",
    "252 dict.ice.g.txt\n",
    "407 dict.ice.p.txt\n",
    "1.6K preprocess.log\n",
    "1.4K test.ice.g-ice.p.ice.g.bin\n",
    "1.2K test.ice.g-ice.p.ice.g.idx\n",
    "1.4K test.ice.g-ice.p.ice.p.bin\n",
    "1.2K test.ice.g-ice.p.ice.p.idx\n",
    "11K train.ice.g-ice.p.ice.g.bin\n",
    "9.5K train.ice.g-ice.p.ice.g.idx\n",
    "11K train.ice.g-ice.p.ice.p.bin\n",
    "9.5K train.ice.g-ice.p.ice.p.idx\n",
    "1.3K valid.ice.g-ice.p.ice.g.bin\n",
    "1.2K valid.ice.g-ice.p.ice.g.idx\n",
    "1.3K valid.ice.g-ice.p.ice.p.bin\n",
    "1.2K valid.ice.g-ice.p.ice.p.idx\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4630ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Training\n",
    "#random number for random seed:\n",
    "#shuf -i 1-100 -n 1\n",
    "#230\n",
    "\n",
    "#these docs have provided some more arguments not found in the fairseq docs page:\n",
    "#https://fairseq.readthedocs.io/en/latest/models.html\n",
    "\n",
    "\n",
    "fairseq-train \\\n",
    "    data-bin \\\n",
    "    --source-lang ice.g \\\n",
    "    --target-lang ice.p \\\n",
    "    --seed 230 \\\n",
    "    --arch lstm \\\n",
    "    --dropout 0.2 \\\n",
    "    --lr .001 \\\n",
    "    --max-update 800\\\n",
    "    --no-epoch-checkpoints \\\n",
    "    --batch-size 50 \\\n",
    "    --clip-norm 1 \\\n",
    "    --label-smoothing .1 \\\n",
    "    --optimizer adam \\\n",
    "    --clip-norm 1 \\\n",
    "    --criterion label_smoothed_cross_entropy\\\n",
    "    --encoder-embed-dim 128 \\\n",
    "    --decoder-embed-dim 128 \\\n",
    "    --encoder-layers 4 \\\n",
    "    --decoder-layers 4 \\\n",
    "    --encoder-embed-dim 512 \\\n",
    "    --decoder-embed-dim 512\n",
    "\n",
    "\n",
    "#The reason I was getting 1.6gb checkpoints instead of 225mb is because I had 512 for the encoder/decoder levels,\n",
    "#not the size of the hidden layer.  Changing that, it sped up and I noticed the loss going down with each epoch.\n",
    "#It all took less than 15 minutes, 8 cores, 32gb RAM. \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d067a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2331002950668335\n",
      "-0.7135706543922424\n",
      "-0.9314574599266052\n",
      "-0.7868616580963135\n",
      "-1.538567304611206\n",
      "-1.6878055334091187\n",
      "-0.11623643338680267\n",
      "-1.046684980392456\n",
      "-1.2919269800186157\n",
      "-0.8849603533744812\n",
      "-0.9858402609825134\n",
      "-0.9449114799499512\n",
      "-0.5537390112876892\n",
      "-0.6720098257064819\n",
      "-1.2061625719070435\n",
      "-0.8228558897972107\n",
      "-0.972842812538147\n",
      "-0.9724523425102234\n",
      "-0.7323872447013855\n",
      "-0.9839916825294495\n",
      "-1.1975090503692627\n",
      "-1.0102899074554443\n",
      "-0.6499191522598267\n",
      "-0.38109153509140015\n",
      "-1.0316599607467651\n",
      "-0.6120491623878479\n",
      "-0.2695963382720947\n",
      "-1.2330626249313354\n",
      "-0.930550217628479\n",
      "-0.8171670436859131\n",
      "-0.8457822799682617\n",
      "-0.6752185225486755\n",
      "-0.3255964517593384\n",
      "-0.7316094636917114\n",
      "-0.8885765075683594\n",
      "-1.012841820716858\n",
      "-0.574596643447876\n",
      "-0.5706310868263245\n",
      "-0.4279146194458008\n",
      "-0.4554976522922516\n",
      "-0.7805471420288086\n",
      "-0.5652311444282532\n",
      "-0.8771812319755554\n",
      "-0.35098350048065186\n",
      "-0.3960602581501007\n",
      "-0.4590819776058197\n",
      "-0.8620086312294006\n",
      "-0.6928106546401978\n",
      "-0.2973653972148895\n",
      "-0.30784809589385986\n",
      "-0.41857412457466125\n",
      "-1.099627137184143\n",
      "-0.12545253336429596\n",
      "-0.2622663080692291\n",
      "-0.27158623933792114\n",
      "-0.7977610230445862\n",
      "-0.16773980855941772\n",
      "-0.20102795958518982\n",
      "-0.15777024626731873\n",
      "-0.11231815069913864\n",
      "-0.4078775644302368\n",
      "-0.14133472740650177\n",
      "-1.5295404195785522\n",
      "-0.799263060092926\n",
      "-0.394481897354126\n",
      "-0.15427885949611664\n",
      "-0.6915934681892395\n",
      "-0.9115318655967712\n",
      "-0.5280343294143677\n",
      "-0.33338022232055664\n",
      "-0.47686076164245605\n",
      "-0.8640428185462952\n",
      "-0.31970182061195374\n",
      "-1.385231375694275\n",
      "-0.5468236804008484\n",
      "-0.30921679735183716\n",
      "-0.22291991114616394\n",
      "-0.19023020565509796\n",
      "-0.26956766843795776\n",
      "-0.18653330206871033\n",
      "-0.5678597092628479\n",
      "-0.1631375104188919\n",
      "-0.5385160446166992\n",
      "-0.7599311470985413\n",
      "-0.28507933020591736\n",
      "-0.5091779232025146\n",
      "-0.603335976600647\n",
      "-0.6482341885566711\n",
      "-0.693988025188446\n",
      "-0.5622348189353943\n",
      "-0.26866528391838074\n",
      "-0.4016633629798889\n",
      "-0.26012086868286133\n",
      "-0.34412524104118347\n",
      "-0.9898154735565186\n",
      "-0.18988138437271118\n",
      "-0.33829572796821594\n",
      "-0.18674491345882416\n",
      "-0.4564518332481384\n",
      "-0.10909247398376465\n"
     ]
    }
   ],
   "source": [
    "#Part 4: Evaluation\n",
    "input_file = \"predictions.txt\"\n",
    "\n",
    "with open(input_file) as to_read:\n",
    "    for line in to_read:\n",
    "        if line.startswith('H-'):\n",
    "            split_string = re.split(\"\\t+\", line)\n",
    "            a = print(str(split_string[1]))\n",
    "\n",
    "#stuck (literally hours - you should see this cell before I cleaned it up!)\n",
    "#cannot fathom how to even get the average from this string\n",
    "#i put it in LibreOffice Calc and got a WER of 46.049 - not great, so I think my calculations are off.\n",
    "\n",
    "\n",
    "#WER = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just out of curiousity...\n",
    "#Bleu Score (some help from https://github.com/pytorch/fairseq/issues/3000)\n",
    "grep ^H gen.out | cut -f3- > gen.out.sys\n",
    "grep ^T gen.out | cut -f2- > gen.out.ref\n",
    "\n",
    "fairseq-score --sys gen.out.sys --ref gen.out.ref\n",
    "\n",
    "Namespace(ignore_case=False, order=4, ref='gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='gen.out.sys')\n",
    "BLEU4 = 47.88, 82.9/59.3/42.8/28.4 (BP=0.969, ratio=0.969, syslen=567, reflen=585)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ed729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PART FIVE: REFLECTION\n",
    "\n",
    "Once I had the correct flags for fairseq-train, it ran smoothly.\n",
    "Thank you so much for the code-suggestions--as I mentioned before, I'm a bit rusty, away from python for over a year.\n",
    "I am going to go through and improve the code with your suggestions.\n",
    "\n",
    "I tend to get stuck looking up EVERY step of text processing; and in the case of the Part Four, I didn't manage\n",
    "to obtain an average from a simple string.  I think it'd be best to convert it to an array that can easily be\n",
    "processed by numpy, but I wasn't able to do that.\n",
    "\n",
    "\n",
    "I very much enjoy working on the command line, so it was a good chance to incorporate the terminal and python for\n",
    "this exercise.  It is quite discouraging to get stuck on trivial tasks like averaging.\n",
    "\n",
    "I am including the predictions.txt file in this assignment folder.\n",
    "I plan to tweak some of the flags and see if I can improve the WER.\n",
    "\n",
    "It's also quite possible that my understanding of the WER is faulty i.e. if the -H average score *100 is not \n",
    "the WER but rather if a _complete_ hypothesized word is the same as the target word.  But even \n",
    "\n",
    "I compiled the NIST SCTK toolkit https://github.com/usnistgov/SCTK just to take a look at it.\n",
    "\n",
    "All in all, I enjoyed trying out Fairseq, I just wish I had had better results.\n",
    "\n",
    "Addendum: when I changed the size of the hidden layer from 512 to 1024, I noticed the following:\n",
    "Quite slower, generating checkpoints over three times as large.\n",
    "The results are similar.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
