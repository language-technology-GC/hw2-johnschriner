{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37dd55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a64b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This assignment will use FairSeq, a neural network sequence-to-sequence learning tool, to build an encoder-decoder LSTM grapheme-to-phoneme engine for Icelandic.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This assignment will use FairSeq, a neural network sequence-to-sequence learning tool, to build an encoder-decoder LSTM grapheme-to-phoneme engine for Icelandic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0967ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "#preprocessing: checked all tsv files for missing fields\n",
    "#preparing first column of the training data for FairSeq\n",
    "trainice = open('train.ice.g', 'w')\n",
    "\n",
    "with open(\"ice_train.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        firstcolumn = (i[0])\n",
    "        print(' '.join(firstcolumn), file = trainice)\n",
    "    trainice.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a4543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing second column of the training data for FairSeq - no spaces added\n",
    "\n",
    "trainice2 = open('train.ice.p', 'w')\n",
    "with open(\"ice_train.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        secondcolumn = (i[1])\n",
    "        print(secondcolumn, file = trainice2)\n",
    "    trainice2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c8f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing first column of the dev data for FairSeq\n",
    "\n",
    "device = open('dev.ice.g', 'w')\n",
    "\n",
    "with open(\"ice_dev.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        firstcolumn = (i[0])\n",
    "        print(' '.join(firstcolumn), file = device)\n",
    "    device.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4687b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing second column of the dev data for FairSeq - no spaces added\n",
    "device2 = open('dev.ice.p', 'w')\n",
    "with open(\"ice_dev.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        secondcolumn = (i[1])\n",
    "        print(secondcolumn, file = device2)\n",
    "    device2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fe31458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing first column of the test data for FairSeq\n",
    "testice = open('test.ice.g', 'w')\n",
    "\n",
    "with open(\"ice_test.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        firstcolumn = (i[0])\n",
    "        print(' '.join(firstcolumn), file = testice)\n",
    "    testice.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2dcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing first column of the test data for FairSeq - no spaces added\n",
    "testice2 = open('test.ice.p', 'w')\n",
    "with open(\"ice_test.tsv\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in reader:\n",
    "        secondcolumn = (i[1])\n",
    "        print(secondcolumn, file = testice2)\n",
    "    testice2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779d8d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LOG from preprocessing:\\ndeb@ubuntuuu:~/Downloads/homework2$ fairseq-preprocess >     --source-lang ice.g >     --target-lang ice.p >     --trainpref train >     --validpref dev >     --testpref test >     --tokenizer space >     --thresholdsrc 2 >     --thresholdtgt 2\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='ice.g', srcdict=None, target_lang='ice.p', task='translation', tensorboard_logdir=None, testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer='space', tpu=False, trainpref='train', user_dir=None, validpref='dev', workers=1)\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] train.ice.g: 800 sents, 5242 tokens, 0.0191% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] dev.ice.g: 100 sents, 634 tokens, 0.0% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] test.ice.g: 100 sents, 667 tokens, 0.0% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] train.ice.p: 800 sents, 5376 tokens, 0.0558% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] dev.ice.p: 100 sents, 652 tokens, 0.153% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] test.ice.p: 100 sents, 685 tokens, 0.292% replaced by <unk>\\n2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\\n\\nFiles created in data-bin:\\n\\nls -l -h| awk '{print $5, $9}'\\n \\n252 dict.ice.g.txt\\n407 dict.ice.p.txt\\n1.6K preprocess.log\\n1.4K test.ice.g-ice.p.ice.g.bin\\n1.2K test.ice.g-ice.p.ice.g.idx\\n1.4K test.ice.g-ice.p.ice.p.bin\\n1.2K test.ice.g-ice.p.ice.p.idx\\n11K train.ice.g-ice.p.ice.g.bin\\n9.5K train.ice.g-ice.p.ice.g.idx\\n11K train.ice.g-ice.p.ice.p.bin\\n9.5K train.ice.g-ice.p.ice.p.idx\\n1.3K valid.ice.g-ice.p.ice.g.bin\\n1.2K valid.ice.g-ice.p.ice.g.idx\\n1.3K valid.ice.g-ice.p.ice.p.bin\\n1.2K valid.ice.g-ice.p.ice.p.idx\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''LOG from preprocessing:\n",
    "deb@ubuntuuu:~/Downloads/homework2$ fairseq-preprocess \\\n",
    ">     --source-lang ice.g \\\n",
    ">     --target-lang ice.p \\\n",
    ">     --trainpref train \\\n",
    ">     --validpref dev \\\n",
    ">     --testpref test \\\n",
    ">     --tokenizer space \\\n",
    ">     --thresholdsrc 2 \\\n",
    ">     --thresholdtgt 2\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='ice.g', srcdict=None, target_lang='ice.p', task='translation', tensorboard_logdir=None, testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer='space', tpu=False, trainpref='train', user_dir=None, validpref='dev', workers=1)\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] train.ice.g: 800 sents, 5242 tokens, 0.0191% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] dev.ice.g: 100 sents, 634 tokens, 0.0% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] Dictionary: 40 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.g] test.ice.g: 100 sents, 667 tokens, 0.0% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] train.ice.p: 800 sents, 5376 tokens, 0.0558% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] dev.ice.p: 100 sents, 652 tokens, 0.153% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] Dictionary: 64 types\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | [ice.p] test.ice.p: 100 sents, 685 tokens, 0.292% replaced by <unk>\n",
    "2021-10-22 14:34:12 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n",
    "\n",
    "Files created in data-bin:\n",
    "\n",
    "ls -l -h| awk '{print $5, $9}'\n",
    " \n",
    "252 dict.ice.g.txt\n",
    "407 dict.ice.p.txt\n",
    "1.6K preprocess.log\n",
    "1.4K test.ice.g-ice.p.ice.g.bin\n",
    "1.2K test.ice.g-ice.p.ice.g.idx\n",
    "1.4K test.ice.g-ice.p.ice.p.bin\n",
    "1.2K test.ice.g-ice.p.ice.p.idx\n",
    "11K train.ice.g-ice.p.ice.g.bin\n",
    "9.5K train.ice.g-ice.p.ice.g.idx\n",
    "11K train.ice.g-ice.p.ice.p.bin\n",
    "9.5K train.ice.g-ice.p.ice.p.idx\n",
    "1.3K valid.ice.g-ice.p.ice.g.bin\n",
    "1.2K valid.ice.g-ice.p.ice.g.idx\n",
    "1.3K valid.ice.g-ice.p.ice.p.bin\n",
    "1.2K valid.ice.g-ice.p.ice.p.idx\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Training\n",
    "#random number for random seed:\n",
    "#shuf -i 1-100 -n 1\n",
    "#230\n",
    "\n",
    "#these docs have provided some more arguments not found in the fairseq docs page:\n",
    "#https://fairseq.readthedocs.io/en/latest/models.html\n",
    "\n",
    "\n",
    "fairseq-train \\\n",
    "    data-bin \\\n",
    "    --source-lang ice.g \\\n",
    "    --target-lang ice.p \\\n",
    "    --seed 230 \\\n",
    "    --arch lstm \\\n",
    "    --dropout 0.2 \\\n",
    "    --lr .001 \\\n",
    "    --max-update 800\\\n",
    "    --no-epoch-checkpoints \\\n",
    "    --batch-size 50 \\\n",
    "    --clip-norm 1 \\\n",
    "    --label-smoothing .1 \\\n",
    "    --optimizer adam \\\n",
    "    --clip-norm 1 \\\n",
    "    --criterion label_smoothed_cross_entropy\\\n",
    "    --encoder-embed-dim 128 \\\n",
    "    --decoder-embed-dim 128 \\\n",
    "    --encoder-layers 512 \\\n",
    "    --decoder-layers 512 \\\n",
    "\n",
    "#It ran through 8 epochs before I halted the 9th at about 45 minutes. (predictions-8_epochs.txt in folder)\n",
    "#During the write-up and evaluation I was curious and got a Bleu score of 0 meaning something definitely went wrong.\n",
    "#My tokenize Hypothesis (H) and my Detokenized are the same \"s a a a a\" throughout the whole file,\n",
    "#so I am going to re-train and re-evaluate.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Evaluation\n",
    "#RE-TRAINING this weekend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bleu Score (some help from https://github.com/pytorch/fairseq/issues/3000)\n",
    "grep ^H gen.out | cut -f3- > gen.out.sys\n",
    "grep ^T gen.out | cut -f2- > gen.out.ref\n",
    "\n",
    "fairseq-score --sys gen.out.sys --ref gen.out.ref\n",
    "\n",
    "BLEU4 = 0.00, 21.7/1.4/0.0/0.0 (BP=0.775, ratio=0.797, syslen=466, reflen=585)\n",
    "#uh oh\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
